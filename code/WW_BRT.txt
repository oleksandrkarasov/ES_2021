command line to run this model:
C:\VisTrails_SAHM\Central_R\R-3.2.0\bin\x64\Rterm.exe --vanilla -f C:\VisTrails_SAHM\vistrails\packages\sahm\pySAHM\Resources\R_Modules\FIT_BRT_pluggable.r --args multicore=TRUE om=2 cur_processing_mode=single models sequentially (n - 1 cores each) c=D:\oleksandr\model\WildlifeWatching\brt_1\CovariateCorrelationOutputMDS_initial.csv o=D:\oleksandr\model\WildlifeWatching\brt_1 mpt=TRUE seed=1234 mes=TRUE rc=responseBinary mbt=TRUE
outDir= D:\oleksandr\model\WildlifeWatching\brt_1

Session continued


R version 3.2.0 (2015-04-16) -- "Full of Ingredients"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ###############################################################################
> ##
> ## Copyright (C) 20010-2012, USGS Fort Collins Science Center. 
> ## All rights reserved.
> ## Contact: talbertc@usgs.gov
> ##
> ## This file is part of the Software for Assisted Habitat Modeling package
> ## for VisTrails.
> ##
> ## "Redistribution and use in source and binary forms, with or without 
> ## modification, are permitted provided that the following conditions are met:
> ##
> ##  - Redistributions of source code must retain the above copyright notice, 
> ##    this list of conditions and the following disclaimer.
> ##  - Redistributions in binary form must reproduce the above copyright 
> ##    notice, this list of conditions and the following disclaimer in the 
> ##    documentation and/or other materials provided with the distribution.
> ##  - Neither the name of the University of Utah nor the names of its 
> ##    contributors may be used to endorse or promote products derived from 
> ##    this software without specific prior written permission.
> ##
> ## THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" 
> ## AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, 
> ## THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR 
> ## PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR 
> ## CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, 
> ## EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, 
> ## PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; 
> ## OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, 
> ## WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR 
> ## OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF 
> ## ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
> ##
> ## Although this program has been used by the U.S. Geological Survey (USGS), 
> ## no warranty, expressed or implied, is made by the USGS or the 
> ## U.S. Government as to the accuracy and functioning of the program and 
> ## related program material nor shall the fact of distribution constitute 
> ## any such warranty, and no responsibility is assumed by the USGS 
> ## in connection therewith.
> ##
> ## Any use of trade, firm, or product names is for descriptive purposes only 
> ## and does not imply endorsement by the U.S. Government.
> ###############################################################################
> 
> make.p.tif=T
> make.binary.tif=T
> 
> tc=NULL
> n.folds=3
> alpha=1
> n.trees=NULL
> learning.rate = NULL
> bag.fraction = 0.75
> prev.stratify = TRUE
> max.trees = 10000
> tolerance.method = "auto"
> tolerance = 0.001
> seed=NULL
> opt.methods=2
> MESS=FALSE
> multCore=TRUE
> predSelect=TRUE
> 
> # Interpret command line argurments #
> # Make Function Call #
> 
> Args <- commandArgs(trailingOnly=FALSE)
> 
>     for (i in 1:length(Args)){
+      if(Args[i]=="-f") ScriptPath<-Args[i+1]
+      argSplit <- strsplit(Args[i], "=")
+      if(argSplit[[1]][1]=="--file") ScriptPath <- argSplit[[1]][2]
+      }
> 
>     for (arg in Args) {
+     	argSplit <- strsplit(arg, "=")
+     	argSplit[[1]][1]
+     	argSplit[[1]][2]
+     	if(argSplit[[1]][1]=="c") csv <- argSplit[[1]][2]
+     	if(argSplit[[1]][1]=="o") output <- argSplit[[1]][2]
+     	if(argSplit[[1]][1]=="rc") responseCol <- argSplit[[1]][2]
+    		if(argSplit[[1]][1]=="mpt") make.p.tif <- as.logical(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="mbt")  make.binary.tif <- as.logical(argSplit[[1]][2])
+       if(argSplit[[1]][1]=="tc")  tc <- as.numeric(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="nf")  n.folds <- as.numeric(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="alp")  alpha <- as.numeric(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="ntr")  n.trees <- as.numeric(argSplit[[1]][2])
+       if(argSplit[[1]][1]=="lr")  learning.rate <- as.numeric(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="bf")  bag.fraction <- as.numeric(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="ps")  prev.stratify <- as.logical(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="mt")  max.trees <- as.numeric(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="om")  opt.methods <- as.numeric(argSplit[[1]][2])
+  			if(argSplit[[1]][1]=="seed")  seed <- as.numeric(argSplit[[1]][2])
+  		  if(argSplit[[1]][1]=="tolm")  tolerance.method <- argSplit[[1]][2]
+  		  if(argSplit[[1]][1]=="tol")  tolerance <- as.numeric(argSplit[[1]][2])
+  		  if(argSplit[[1]][1]=="mes")  MESS <-as.logical(argSplit[[1]][2])
+ 	    if(argSplit[[1]][1]=="multicore")  multCore <- as.logical(argSplit[[1]][2])
+    	  if(argSplit[[1]][1]=="pst")  predSelect <- as.logical(argSplit[[1]][2])
+  			
+     }
> 
> ScriptPath<-dirname(ScriptPath)
> source(file.path(ScriptPath,"LoadRequiredCode.r"))
> source(file.path(ScriptPath,"BRT.helper.fcts.r"))
> 
>  
> 
>     FitModels(ma.name=csv,
+ 		output.dir=output,
+ 		response.col=responseCol,
+ 		make.p.tif=make.p.tif,make.binary.tif=make.binary.tif,
+ 		simp.method="cross-validation",debug.mode=F,tc=tc,n.trees=n.trees, n.folds=n.folds,alpha=alpha,script.name="brt",
+ 		learning.rate =learning.rate, bag.fraction = bag.fraction,prev.stratify = prev.stratify,max.trees = max.trees,seed=seed,
+     opt.methods=opt.methods,MESS=MESS,tolerance.method = tolerance.method,tolerance=tolerance,ScriptPath=ScriptPath,multCore=multCore,predSelect=predSelect)

tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 200
lr = 0.05   optimal n trees = 200 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 900
lr = 0.01   optimal n trees = 900 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0851 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.0999(0.037)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 
dropping predictor 4 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - l8lumi_prom_21 
4 - l8swir1_gearys_7 

50%


Settings:

	random seed used             : 1234
	tree complexity              : 16
	learning rate                : 0.0851
	n(trees)                     : 300
	model simplification         : cross-validation
	n folds                      : 3
	n covariates in final model  : 6
Relative influence of predictors in final model:

                     Var  rel.inf
4        s1ratio_prom_21 20.18779
5      s2glc_contrast_21 18.88176
3 l8tcap_brightness_sd_7 16.90971
6           s2glc_dent_7 15.97989
1         alos_imcorr1_7 15.83263
2         l8nir_gearys_7 12.20822

Important interactions in final model:

 v1           name1 v2                  name2
  6    s2glc_dent_7  5      s2glc_contrast_21
  4 s1ratio_prom_21  3 l8tcap_brightness_sd_7
1  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 100
lr = 0.05   optimal n trees = 100 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0778 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.1089(0.0366)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 
dropping predictor 4 
dropping predictor 5 
dropping predictor 6 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - l8nir_gearys_7 
4 - s1ratio_prom_21 
5 - alos_imcorr1_7 
6 - l8lumi_prom_21 

50%
2  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 100
lr = 0.05   optimal n trees = 100 
[1] 200
lr = 0.02   optimal n trees = 200 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0716 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.0998(0.0287)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - l8swir1_gearys_7 

50%
3  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 100
lr = 0.05   optimal n trees = 100 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 1000
lr = 0.01   optimal n trees = 1000 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0743 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.1266(0.0316)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - s1ratio_prom_21 

50%
4  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 200
lr = 0.05   optimal n trees = 200 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0887 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.0899(0.0167)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 

50%
5  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 100
lr = 0.05   optimal n trees = 100 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0778 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.084(0.0168)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - l8lumi_prom_21 

50%
6  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 100
lr = 0.05   optimal n trees = 100 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0778 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.0971(0.0095)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 

50%
7  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 200
lr = 0.05   optimal n trees = 200 
[1] 200
lr = 0.02   optimal n trees = 200 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0825 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.1107(0.0444)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 
dropping predictor 4 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - l8swir1_gearys_7 
4 - l8lumi_prom_21 

50%
8  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 100
lr = 0.05   optimal n trees = 100 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0778 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.0748(0.0216)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 
dropping predictor 4 
dropping predictor 5 
dropping predictor 6 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - l8swir1_gearys_7 
4 - l8lumi_prom_21 
5 - alos_imcorr1_7 
6 - l8nir_gearys_7 

50%
9  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 100
lr = 0.05   optimal n trees = 100 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0778 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.1086(0.0543)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 
dropping predictor 3 
dropping predictor 4 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 
3 - s1ratio_prom_21 
4 - l8nir_gearys_7 

50%
10  
tree complexity set to 4 
[1] 100
lr = 0.1   optimal n trees = 100 
[1] 200
lr = 0.05   optimal n trees = 200 
[1] 400
lr = 0.02   optimal n trees = 400 
[1] 800
lr = 0.01   optimal n trees = 800 

finished with learning rate estimation, lr= 0.01
for final fit, lr= 0.0887 and tc= 16 

gbm.simplify - version 2.9 

simplifying gbm.step model for response with 10 predictors and 1000 observations 
original deviance = 1.1039(0.0156)

variable removal will proceed until average change exceeds the original se

creating initial models...
done 

dropping predictor 1 
dropping predictor 2 

now processing final dropping of variables with full data 

1 - s2glc_prom_7 
2 - l8sat_dent_7 

50%
 
40%
Progress:70%

finished with final model summarization, t= 14.39 sec
Progress:80%

producing prediction maps... 
 

finished with prediction maps, t= 1163.77 sec
Progress:100%
> 
> 
> 
> 

tiles_dname D:\oleksandr\model\WildlifeWatching\brt_1\BinTiff
outFname D:\oleksandr\model\WildlifeWatching\brt_1\brt_bin_map.tif
0...10...20...30...40...50...60...70...80...90...100 - done.
tiles_dname D:\oleksandr\model\WildlifeWatching\brt_1\ProbTiff
outFname D:\oleksandr\model\WildlifeWatching\brt_1\brt_prob_map.tif
0...10...20...30...40...50...60...70...80...90...100 - done.
tiles_dname D:\oleksandr\model\WildlifeWatching\brt_1\ResidTiff
outFname D:\oleksandr\model\WildlifeWatching\brt_1\brt_resid_map.tif
0...10...20...30...40...50...60...70...80...90...100 - done.
tiles_dname D:\oleksandr\model\WildlifeWatching\brt_1\MESSTiff
outFname D:\oleksandr\model\WildlifeWatching\brt_1\brt_mess_map.tif
0...10...20...30...40...50...60...70...80...90...100 - done.
tiles_dname D:\oleksandr\model\WildlifeWatching\brt_1\ModTiff
outFname D:\oleksandr\model\WildlifeWatching\brt_1\brt_mod_map.tif
0...10...20...30...40...50...60...70...80...90...100 - done.
Finished successfully!
